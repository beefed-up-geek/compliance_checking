{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad9e0e3c764e487c9ea5183a4970e127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bdd4b22060a4aa8a5f82b3daebb458e",
              "IPY_MODEL_768f821e683a45428fe086e8599f6233",
              "IPY_MODEL_8f854f1510bb4ec1955b7f3d91349cda"
            ],
            "layout": "IPY_MODEL_c89b626891d34d9685251d173f706ba7"
          }
        },
        "0bdd4b22060a4aa8a5f82b3daebb458e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d6d197feff54f929d7b8b55221125e8",
            "placeholder": "​",
            "style": "IPY_MODEL_1600a789043b48ffa211d9b6a4612dba",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "768f821e683a45428fe086e8599f6233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_975a15e062324e1faf91c158345a9d82",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f71a5cc0ec345c89f3b4607921e30b3",
            "value": 4
          }
        },
        "8f854f1510bb4ec1955b7f3d91349cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d1e52634be4859ac67da3bf5252c08",
            "placeholder": "​",
            "style": "IPY_MODEL_2d865d1896c44a28bfee7ab85786753d",
            "value": " 4/4 [00:04&lt;00:00,  1.02s/it]"
          }
        },
        "c89b626891d34d9685251d173f706ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6d197feff54f929d7b8b55221125e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1600a789043b48ffa211d9b6a4612dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "975a15e062324e1faf91c158345a9d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f71a5cc0ec345c89f3b4607921e30b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18d1e52634be4859ac67da3bf5252c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d865d1896c44a28bfee7ab85786753d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation 스위치"
      ],
      "metadata": {
        "id": "oLPMYfphVjoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TERM_DEFINITION_ON = True\n",
        "ENTITY_ON = True\n",
        "CONCEPT_ON = True\n",
        "EVENTIC_ON = True\n",
        "FULL_GRAPH_ON = True # 이게 True면 위의 모든 스위치를 무시하고 모든 내용이 포함됨"
      ],
      "metadata": {
        "id": "GbARFGmvVmrD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 런타임 준비 & 필수 라이브러리 설치 & 허깅페이스 로그인"
      ],
      "metadata": {
        "id": "9eiYASS1RGll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEbaexqNQdvC",
        "outputId": "d46006ac-fbf0-4705-8ee0-1f800642216f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Git LFS initialized.\n",
            "Cloning into 'compliance_checking'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 109 (delta 20), reused 98 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (109/109), 9.36 MiB | 17.94 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "Filtering content: 100% (7/7), 530.53 MiB | 18.33 MiB/s, done.\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `250722_HCLT_KACL_2025` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `250722_HCLT_KACL_2025`\n"
          ]
        }
      ],
      "source": [
        "!pip -q install \"transformers>=4.43.0\" accelerate bitsandbytes \"scikit-learn>=1.3.0\" pandas tqdm\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y git-lfs\n",
        "!git lfs install\n",
        "!rm -rf compliance_checking\n",
        "!git clone https://github.com/beefed-up-geek/compliance_checking.git\n",
        "!pip -q install -U \"huggingface_hub[cli]\" transformers accelerate\n",
        "\n",
        "import torch, sys, os, re, json, math, random, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 로드 (fp16)"
      ],
      "metadata": {
        "id": "rqiQcDJCRJ8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Qwen/Qwen2-7B-Instruct, google/gemma-7b-it, meta-llama/Llama-3.1-8B-Instruct\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# pad 토큰 설정(경고 방지)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loaded:\", MODEL_ID)\n",
        "print(\"Model dtype:\", model.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "ad9e0e3c764e487c9ea5183a4970e127",
            "0bdd4b22060a4aa8a5f82b3daebb458e",
            "768f821e683a45428fe086e8599f6233",
            "8f854f1510bb4ec1955b7f3d91349cda",
            "c89b626891d34d9685251d173f706ba7",
            "4d6d197feff54f929d7b8b55221125e8",
            "1600a789043b48ffa211d9b6a4612dba",
            "975a15e062324e1faf91c158345a9d82",
            "4f71a5cc0ec345c89f3b4607921e30b3",
            "18d1e52634be4859ac67da3bf5252c08",
            "2d865d1896c44a28bfee7ab85786753d"
          ]
        },
        "id": "mG9cc2IgRZoo",
        "outputId": "11f7a6a4-7361-4b3c-c9db-f03df7281639"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad9e0e3c764e487c9ea5183a4970e127"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: meta-llama/Llama-3.1-8B-Instruct\n",
            "Model dtype: torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 불러오기"
      ],
      "metadata": {
        "id": "KPnI1F1jRbZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "LOCAL_PATH = \"compliance_checking/environments/CONTRACT/data/manipulated/contract_norms_fusion_graph.json\"\n",
        "assert os.path.exists(LOCAL_PATH), \"파일 경로를 확인하세요.\"\n",
        "\n",
        "with open(LOCAL_PATH, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"총 샘플 수: {len(data)}\")\n",
        "print(json.dumps(data[0], ensure_ascii=False, indent=2)[:1200], \"...\\n\")\n"
      ],
      "metadata": {
        "id": "k6bmlV9pReIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209aa898-7843-4dfe-aaff-1bd573037f6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수: 198\n",
            "{\n",
            "  \"contract_id\": 466,\n",
            "  \"norm1\": {\n",
            "    \"norm_id\": 82524,\n",
            "    \"norm_text\": \"To this end, subject to any confidentiality agreements Solectron may have, Solectron will both inform and provide a commercially reasonable opportunity for acquisition of new and emerging Solectron and industry technology.\",\n",
            "    \"fusion_graph\": {\n",
            "      \"edges\": [\n",
            "        {\n",
            "          \"source\": \"Solectron\",\n",
            "          \"relation\": \"will\",\n",
            "          \"target\": \"inform acquisition of new and emerging Solectron and industry technology subject to confidentiality agreements\",\n",
            "          \"source_graph\": \"eventic\"\n",
            "        },\n",
            "        {\n",
            "          \"source\": \"Solectron\",\n",
            "          \"relation\": \"will\",\n",
            "          \"target\": \"provide opportunity for acquisition of new and emerging Solectron and industry technology subject to confidentiality agreements\",\n",
            "          \"source_graph\": \"eventic\"\n",
            "        },\n",
            "        {\n",
            "          \"source\": \"Solectron\",\n",
            "          \"relation\": \"successor\",\n",
            "          \"target\": \"Flextronics\",\n",
            "          \"source_graph\": \"entity\"\n",
            "        },\n",
            "        {\n",
            "          \"source\": \"Solectron\",\n",
            "          \"relation\": \"industry\",\n",
            "          \"target\": \"Electronics Manufacturing Services\",\n",
            "          \"source_graph\": \"entity\"\n",
            "      ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프롬프트 빌더 & 생성 함수"
      ],
      "metadata": {
        "id": "NGduFN2YRfo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a precise contract-compliance analyst.\n",
        "Given two norms from the same contract and their fused graphs, decide if they CONFLICT.\n",
        "Output strictly one digit: 1 if they conflict (mutually unsatisfiable under the same conditions), 0 otherwise.\n",
        "Do not add any words or punctuation. Only '1' or '0'.\"\"\"\n",
        "\n",
        "def filter_edges_by_flags(edges):\n",
        "    \"\"\"\n",
        "    Ablation 스위치에 따라 edge 리스트를 필터링.\n",
        "    - FULL_GRAPH_ON=True면 그대로 반환 (모든 source_graph 포함).\n",
        "    - False면 각 스위치(EVENTIC/ENTITY/TERM_DEFINITION/CONCEPT)에 따라 필터링.\n",
        "    \"\"\"\n",
        "    if edges is None:\n",
        "        return []\n",
        "    if FULL_GRAPH_ON:\n",
        "        return edges\n",
        "\n",
        "    allowed = set()\n",
        "    if EVENTIC_ON:\n",
        "        allowed.add(\"eventic\")\n",
        "    if ENTITY_ON:\n",
        "        allowed.add(\"entity\")\n",
        "    if TERM_DEFINITION_ON:\n",
        "        allowed.add(\"term_definition\")\n",
        "    if CONCEPT_ON:\n",
        "        allowed.add(\"concept\")\n",
        "\n",
        "    out = []\n",
        "    for e in edges:\n",
        "        sg = str(e.get(\"source_graph\", \"\")).strip().lower()\n",
        "        if sg in allowed:\n",
        "            out.append(e)\n",
        "    return out\n",
        "\n",
        "def edges_to_lines(edges, max_edges=None):\n",
        "    lines = []\n",
        "    filt = filter_edges_by_flags(edges)\n",
        "    use = filt if (max_edges is None) else filt[:max_edges]\n",
        "    for e in use:\n",
        "        s = str(e.get(\"source\",\"\")).strip()\n",
        "        r = str(e.get(\"relation\",\"\")).strip()\n",
        "        t = str(e.get(\"target\",\"\")).strip()\n",
        "        lines.append(f\"[{s}] [{r}] [{t}]\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def build_user_prompt(sample, max_edges_per_norm=60):\n",
        "    n1 = sample.get(\"norm1\", {})\n",
        "    n2 = sample.get(\"norm2\", {})\n",
        "    n1_id = n1.get(\"norm_id\", \"\")\n",
        "    n2_id = n2.get(\"norm_id\", \"\")\n",
        "    n1_text = (n1.get(\"norm_text\") or \"\").strip()\n",
        "    n2_text = (n2.get(\"norm_text\") or \"\").strip()\n",
        "    n1_edges = (n1.get(\"fusion_graph\", {}) or {}).get(\"edges\", [])\n",
        "    n2_edges = (n2.get(\"fusion_graph\", {}) or {}).get(\"edges\", [])\n",
        "\n",
        "    # 프롬프트 상단에 어떤 그래프 컴포넌트를 포함했는지 명시\n",
        "    if FULL_GRAPH_ON:\n",
        "        components_note = \"Included graph components: ALL (full graph: eventic, entity, term_definition, concept)\"\n",
        "    else:\n",
        "        included = []\n",
        "        if EVENTIC_ON: included.append(\"eventic\")\n",
        "        if ENTITY_ON: included.append(\"entity\")\n",
        "        if TERM_DEFINITION_ON: included.append(\"term_definition\")\n",
        "        if CONCEPT_ON: included.append(\"concept\")\n",
        "        if not included:\n",
        "            components_note = \"Included graph components: NONE (edges omitted by ablation)\"\n",
        "        else:\n",
        "            components_note = \"Included graph components: \" + \", \".join(included)\n",
        "\n",
        "    n1_lines = edges_to_lines(n1_edges, max_edges=max_edges_per_norm)\n",
        "    n2_lines = edges_to_lines(n2_edges, max_edges=max_edges_per_norm)\n",
        "\n",
        "    user = f\"\"\"Decide whether the following two norms conflict.\n",
        "{components_note}\n",
        "\n",
        "Norm 1 (id={n1_id}):\n",
        "{n1_text}\n",
        "\n",
        "Norm 2 (id={n2_id}):\n",
        "{n2_text}\n",
        "\n",
        "Fusion Graph (Norm 1) — one per line: [source] [relation] [target]\n",
        "{n1_lines}\n",
        "\n",
        "Fusion Graph (Norm 2) — one per line: [source] [relation] [target]\n",
        "{n2_lines}\n",
        "\n",
        "Labeling rule:\n",
        "- Output 1 if the two norms impose incompatible duties/permissions about the same agent/action under the same conditions such that both cannot be satisfied at once.\n",
        "- Output 0 otherwise (e.g., different scope/actors/timing, or both can be satisfied).\n",
        "\n",
        "Answer with ONLY one digit: 1 or 0.\n",
        "\"\"\"\n",
        "    return user\n",
        "\n",
        "\n",
        "import re\n",
        "import torch\n",
        "\n",
        "@torch.inference_mode()\n",
        "def predict_label(sample, max_new_tokens=3, temperature=0.0):\n",
        "    messages = [\n",
        "        {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\":\"user\", \"content\": build_user_prompt(sample)}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False if temperature==0.0 else True,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
        "    out = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # 첫 번째로 보이는 0/1만 사용\n",
        "    m = re.search(r\"[01]\", out)\n",
        "    if m:\n",
        "        return int(m.group(0)), out\n",
        "    return 0, out\n"
      ],
      "metadata": {
        "id": "uD_fHf_QRy2R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True False\n",
        "TERM_DEFINITION_ON = True\n",
        "ENTITY_ON = True\n",
        "CONCEPT_ON = True\n",
        "EVENTIC_ON = True\n",
        "FULL_GRAPH_ON = True # 이게 True면 위의 모든 스위치를 무시하고 모든 내용이 포함됨"
      ],
      "metadata": {
        "id": "WnkfGeT8Y11W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전수 추론 루프 (tqdm 진행바)"
      ],
      "metadata": {
        "id": "0T4kp9BbTDSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "golds, preds, raw_outputs = [], [], []\n",
        "\n",
        "# fp16에서 메모리 여유가 없다면 60 → 40/30으로 낮추세요.\n",
        "max_edges_per_norm = 60\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for sample in tqdm(data, desc=\"LLM inference\"):\n",
        "    if \"norm1\" not in sample or \"norm2\" not in sample or \"conflict\" not in sample:\n",
        "        continue\n",
        "\n",
        "    y_pred, raw = predict_label(sample, max_new_tokens=3, temperature=0.0)\n",
        "    y_true = int(sample[\"conflict\"])\n",
        "\n",
        "    preds.append(y_pred)\n",
        "    golds.append(y_true)\n",
        "    raw_outputs.append(raw)\n",
        "\n",
        "print(\"샘플 수(유효):\", len(golds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_t6k4J4TEbx",
        "outputId": "60a8cb52-4e1a-4982-8e5c-44d85f59c9f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LLM inference:   0%|          | 0/198 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   1%|          | 1/198 [00:00<02:25,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   1%|          | 2/198 [00:00<01:11,  2.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   2%|▏         | 3/198 [00:00<00:49,  3.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   2%|▏         | 4/198 [00:01<00:37,  5.21it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   3%|▎         | 5/198 [00:01<00:33,  5.71it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   3%|▎         | 6/198 [00:01<00:30,  6.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   4%|▎         | 7/198 [00:01<00:28,  6.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   4%|▍         | 8/198 [00:01<00:26,  7.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   5%|▍         | 9/198 [00:01<00:27,  6.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   5%|▌         | 10/198 [00:01<00:27,  6.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   6%|▌         | 11/198 [00:02<00:26,  7.06it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   6%|▌         | 12/198 [00:02<00:24,  7.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   7%|▋         | 13/198 [00:02<00:25,  7.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   7%|▋         | 14/198 [00:02<00:23,  7.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   8%|▊         | 16/198 [00:02<00:21,  8.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   9%|▊         | 17/198 [00:02<00:21,  8.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:   9%|▉         | 18/198 [00:02<00:22,  7.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  10%|▉         | 19/198 [00:02<00:22,  8.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  10%|█         | 20/198 [00:03<00:22,  7.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  11%|█         | 21/198 [00:03<00:21,  8.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  11%|█         | 22/198 [00:03<00:20,  8.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  12%|█▏        | 23/198 [00:03<00:20,  8.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  12%|█▏        | 24/198 [00:03<00:21,  7.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  13%|█▎        | 26/198 [00:03<00:19,  8.61it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  14%|█▍        | 28/198 [00:04<00:19,  8.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  15%|█▍        | 29/198 [00:04<00:19,  8.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  15%|█▌        | 30/198 [00:04<00:20,  8.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  16%|█▌        | 31/198 [00:04<00:20,  8.17it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  16%|█▌        | 32/198 [00:04<00:21,  7.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  17%|█▋        | 33/198 [00:04<00:20,  8.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  17%|█▋        | 34/198 [00:04<00:19,  8.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  18%|█▊        | 36/198 [00:04<00:18,  9.00it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  19%|█▊        | 37/198 [00:05<00:17,  9.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  19%|█▉        | 38/198 [00:05<00:18,  8.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  20%|█▉        | 39/198 [00:05<00:17,  8.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  20%|██        | 40/198 [00:05<00:18,  8.41it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  21%|██        | 41/198 [00:05<00:19,  7.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  21%|██        | 42/198 [00:05<00:20,  7.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  22%|██▏       | 43/198 [00:05<00:19,  8.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  22%|██▏       | 44/198 [00:05<00:18,  8.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  23%|██▎       | 45/198 [00:06<00:18,  8.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  23%|██▎       | 46/198 [00:06<00:18,  8.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  24%|██▎       | 47/198 [00:06<00:18,  7.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  24%|██▍       | 48/198 [00:06<00:18,  8.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  25%|██▍       | 49/198 [00:06<00:18,  8.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  25%|██▌       | 50/198 [00:06<00:17,  8.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  26%|██▌       | 51/198 [00:06<00:17,  8.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  27%|██▋       | 53/198 [00:06<00:16,  8.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  27%|██▋       | 54/198 [00:07<00:16,  8.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  28%|██▊       | 55/198 [00:07<00:16,  8.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  28%|██▊       | 56/198 [00:07<00:16,  8.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  29%|██▉       | 57/198 [00:07<00:17,  8.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  29%|██▉       | 58/198 [00:07<00:16,  8.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  30%|██▉       | 59/198 [00:07<00:16,  8.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  31%|███       | 61/198 [00:07<00:15,  8.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  31%|███▏      | 62/198 [00:08<00:15,  8.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  32%|███▏      | 64/198 [00:08<00:14,  9.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  33%|███▎      | 66/198 [00:08<00:13,  9.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  34%|███▍      | 67/198 [00:08<00:13,  9.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  35%|███▍      | 69/198 [00:08<00:13,  9.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  35%|███▌      | 70/198 [00:08<00:13,  9.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  36%|███▋      | 72/198 [00:09<00:12,  9.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  37%|███▋      | 74/198 [00:09<00:12,  9.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  38%|███▊      | 75/198 [00:09<00:12,  9.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  38%|███▊      | 76/198 [00:09<00:13,  9.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  39%|███▉      | 77/198 [00:09<00:13,  8.92it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  39%|███▉      | 78/198 [00:09<00:13,  8.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  40%|███▉      | 79/198 [00:09<00:13,  8.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  40%|████      | 80/198 [00:09<00:14,  8.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  41%|████      | 81/198 [00:10<00:13,  8.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  41%|████▏     | 82/198 [00:10<00:14,  8.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  42%|████▏     | 83/198 [00:10<00:14,  7.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  42%|████▏     | 84/198 [00:10<00:13,  8.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  43%|████▎     | 85/198 [00:10<00:15,  7.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  43%|████▎     | 86/198 [00:10<00:15,  7.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  44%|████▍     | 87/198 [00:10<00:14,  7.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  44%|████▍     | 88/198 [00:11<00:14,  7.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  45%|████▍     | 89/198 [00:11<00:13,  8.17it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  45%|████▌     | 90/198 [00:11<00:12,  8.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  46%|████▌     | 91/198 [00:11<00:12,  8.69it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  47%|████▋     | 93/198 [00:11<00:11,  9.45it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  47%|████▋     | 94/198 [00:11<00:10,  9.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  48%|████▊     | 95/198 [00:11<00:10,  9.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  48%|████▊     | 96/198 [00:11<00:10,  9.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  49%|████▉     | 97/198 [00:11<00:10,  9.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  49%|████▉     | 98/198 [00:12<00:10,  9.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  50%|█████     | 99/198 [00:12<00:10,  9.38it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  51%|█████     | 100/198 [00:12<00:10,  9.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  52%|█████▏    | 102/198 [00:12<00:09, 10.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  52%|█████▏    | 103/198 [00:12<00:09,  9.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  53%|█████▎    | 105/198 [00:12<00:08, 10.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  54%|█████▍    | 107/198 [00:12<00:08, 10.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  55%|█████▌    | 109/198 [00:13<00:08, 10.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  56%|█████▌    | 111/198 [00:13<00:08, 10.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  57%|█████▋    | 113/198 [00:13<00:08,  9.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  58%|█████▊    | 115/198 [00:13<00:08, 10.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  59%|█████▉    | 117/198 [00:13<00:07, 10.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  60%|██████    | 119/198 [00:14<00:07, 10.77it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  61%|██████    | 121/198 [00:14<00:07, 10.64it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  62%|██████▏   | 123/198 [00:14<00:07, 10.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  63%|██████▎   | 125/198 [00:14<00:07, 10.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  64%|██████▍   | 127/198 [00:14<00:06, 10.48it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  65%|██████▌   | 129/198 [00:15<00:06, 10.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  66%|██████▌   | 131/198 [00:15<00:06, 10.69it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  67%|██████▋   | 133/198 [00:15<00:05, 10.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  68%|██████▊   | 135/198 [00:15<00:05, 11.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  69%|██████▉   | 137/198 [00:15<00:05, 10.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  70%|███████   | 139/198 [00:15<00:05, 11.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  71%|███████   | 141/198 [00:16<00:05, 10.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  72%|███████▏  | 143/198 [00:16<00:05, 10.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  73%|███████▎  | 145/198 [00:16<00:04, 11.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  74%|███████▍  | 147/198 [00:16<00:04, 10.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  75%|███████▌  | 149/198 [00:16<00:04, 11.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  76%|███████▋  | 151/198 [00:17<00:04, 10.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  77%|███████▋  | 153/198 [00:17<00:04, 10.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  78%|███████▊  | 155/198 [00:17<00:04, 10.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  79%|███████▉  | 157/198 [00:17<00:04, 10.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  80%|████████  | 159/198 [00:17<00:03, 10.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  81%|████████▏ | 161/198 [00:18<00:03,  9.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  82%|████████▏ | 163/198 [00:18<00:03,  9.91it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  83%|████████▎ | 164/198 [00:18<00:03,  9.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  84%|████████▍ | 166/198 [00:18<00:03, 10.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  85%|████████▍ | 168/198 [00:18<00:02, 10.61it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  86%|████████▌ | 170/198 [00:18<00:02, 10.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  87%|████████▋ | 172/198 [00:19<00:02, 10.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  88%|████████▊ | 174/198 [00:19<00:02, 11.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  89%|████████▉ | 176/198 [00:19<00:02, 10.92it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  90%|████████▉ | 178/198 [00:19<00:01, 11.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  91%|█████████ | 180/198 [00:19<00:01, 10.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  92%|█████████▏| 182/198 [00:20<00:01, 11.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  93%|█████████▎| 184/198 [00:20<00:01, 11.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  94%|█████████▍| 186/198 [00:20<00:01, 11.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  95%|█████████▍| 188/198 [00:20<00:00, 11.21it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  96%|█████████▌| 190/198 [00:20<00:00, 11.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  97%|█████████▋| 192/198 [00:20<00:00, 11.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  98%|█████████▊| 194/198 [00:21<00:00, 11.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference:  99%|█████████▉| 196/198 [00:21<00:00, 11.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "LLM inference: 100%|██████████| 198/198 [00:21<00:00,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 수(유효): 198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 평가지표 계산 (Accuracy & MCC)"
      ],
      "metadata": {
        "id": "YLH3UEj2TIPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
        "\n",
        "acc = accuracy_score(golds, preds)\n",
        "mcc = matthews_corrcoef(golds, preds)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-oSpnqVTJPg",
        "outputId": "56e1b4f9-dd25-4d4b-a610-5653df2998d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6818\n",
            "Matthews Correlation Coefficient: 0.4714\n"
          ]
        }
      ]
    }
  ]
}